{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Detection and Tracking Project\n",
    "\n",
    "### Project Details\n",
    "For this project, a labeled dataset was provide and my job is to decide what features to extract, then train a classifier and ultimately track vehicles in a video stream. Here are links to the labeled data for [vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) examples to train your classifier. These example images come from a combination of the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself.\n",
    "\n",
    "Udacity recently released a labeled dataset which can be used to take advantage of to augment the training data. The Udacity data can find [here](https://github.com/udacity/self-driving-car/tree/master/annotations). In each of the folders containing images there's a csv file containing all the labels and bounding boxes. To add vehicle images to the training data, we can use the csv files to extract the bounding box regions and scale them to the same size as the rest of the training images.\n",
    "\n",
    "The project video will be the same one as for the Advanced Lane Finding Project. \n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* [Step 1](#step1): Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier.\n",
    "* [Step 2](#step2): Optionally, apply a color transform and append binned color features, as well as histograms of color, to HOG feature vector. \n",
    "* [Step 3](#step3): Normalize the features and randomize a selection for training and testing.\n",
    "* [Step 4](#step4): Implement a sliding-window technique and use trained classifier to search for vehicles in images.\n",
    "* [Step 5](#step5): Run the pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* [Step 6](#step6): Estimate a bounding box for vehicles detected.\n",
    "\n",
    "[//]: # (Image References)\n",
    "[image1]: ./examples/car_not_car.png\n",
    "[image2]: ./examples/HOG_example.jpg\n",
    "[image3]: ./examples/sliding_windows.jpg\n",
    "[image4]: ./examples/sliding_window.jpg\n",
    "[image5]: ./examples/bboxes_and_heat.png\n",
    "[image6]: ./examples/labels_map.png\n",
    "[image7]: ./examples/output_bboxes.png\n",
    "[video1]: ./project_video.mp4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary  libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline (single images)\n",
    "<a id='step1'></a>\n",
    "### Step 1 Histogram of Oriented Gradients (HOG)\n",
    "####1. Explain how (and identify where in your code) you extracted HOG features from the training images.\n",
    "\n",
    "The code for this step is contained in the first code cell of the IPython notebook (or in lines # through # of the file called `some_file.py`).  \n",
    "\n",
    "I started by reading in all the `vehicle` and `non-vehicle` images.  Here is an example of one of each of the `vehicle` and `non-vehicle` classes:\n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "I then explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n",
    "\n",
    "Here is an example using the `YCrCb` color space and HOG parameters of `orientations=8`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n",
    "\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "####2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "I tried various combinations of parameters and...\n",
    "\n",
    "####3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n",
    "I trained a linear SVM using..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2 Optionally, apply a color transform and append binned color features, as well as histograms of color, to HOG feature vector.\n",
    "\n",
    "In this step, I computed the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` from  `objpoints` and `imgpoints` the I applied the distortion correction to the test images using the `cv2.undistort()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "### Step 3 Normalize the features and randomize a selection for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of color and gradient thresholdings was deployed using the `pipeline()` function as below. Other gradient thresholding and color tranform functions are provided in the lectures for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='step4'></a>\n",
    "### Step 4 Sliding Window Search\n",
    "\n",
    "####1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "I decided to search random window positions at random scales all over the image and came up with this (ok just kidding I didn't actually ;):\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "####2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?\n",
    "\n",
    "Ultimately I searched on two scales using YCrCb 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.  Here are some example images:\n",
    "\n",
    "![alt text][image4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Video Implementation\n",
    "<a id='step5'></a>\n",
    "### Step 5 Run the pipeline on a video stream and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "\n",
    "####1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)\n",
    "Here's a [link to my video result](./project_video.mp4)\n",
    "\n",
    "\n",
    "####2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "\n",
    "I recorded the positions of positive detections in each frame of the video.  From the positive detections I created a heatmap and then thresholded that map to identify vehicle positions.  I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.  I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected.  \n",
    "\n",
    "Here's an example result showing the heatmap from a series of frames of video, the result of `scipy.ndimage.measurements.label()` and the bounding boxes then overlaid on the last frame of video:\n",
    "\n",
    "### Here are six frames and their corresponding heatmaps:\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "### Here is the output of `scipy.ndimage.measurements.label()` on the integrated heatmap from all six frames:\n",
    "![alt text][image6]\n",
    "\n",
    "### Here the resulting bounding boxes are drawn onto the last frame in the series:\n",
    "![alt text][image7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step6'></a>\n",
    "### Step 6 Estimate a bounding box for vehicles detected.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    # NOTE: The output you return should be a color image (3 channel) for processing video below\n",
    "    # TODO: put your pipeline here,\n",
    "    # you should return the final output (image where lines are drawn on lanes)\n",
    "\n",
    "    # Step 2 Apply distortion correction to the raw image\n",
    "    # ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[1::-1], None, None)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx) \n",
    "    \n",
    "    # Step 3 Use color transforms, gradients, etc., to create a thresholded binary image\n",
    "    kernel_size = 5\n",
    "    color_binary, combined_binary = pipeline(undist)   \n",
    "    combined_binary = np.uint8(combined_binary * 255)\n",
    "\n",
    "    # Step 4 Apply a perspective transform to rectify binary image (\"birds-eye view\") \n",
    "    img_size = (undist.shape[1], undist.shape[0])\n",
    "    src = np.float32(\n",
    "        [[(img_size[0] / 2) - 55, img_size[1] / 2 + 100],\n",
    "        [((img_size[0] / 6) - 10), img_size[1]],\n",
    "        [(img_size[0] * 5 / 6) + 40, img_size[1]],\n",
    "        [(img_size[0] / 2 + 60), img_size[1] / 2 + 100]])\n",
    "    dst = np.float32(\n",
    "        [[(img_size[0] / 5), 0],\n",
    "        [(img_size[0] / 5), img_size[1]],\n",
    "        [(img_size[0] * 4 / 5), img_size[1]],\n",
    "        [(img_size[0] * 4 / 5), 0]])\n",
    "    \n",
    "    warped, perspective_M = warper(combined_binary, src, dst)\n",
    "\n",
    "    # Step 5 Detect lane pixels and fit to find the lane boundary\n",
    "    binary_warped = np.uint8(warped/255)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src) \n",
    "    result = detect_lane(binary_warped, undist, Minv, is_plotting=False)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video ../project_video_out.mp4\n",
      "[MoviePy] Writing video ../project_video_out.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [07:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: ../project_video_out.mp4 \n",
      "\n",
      "CPU times: user 6min 44s, sys: 1min 59s, total: 8min 44s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "white_output = '../project_video_out.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1 = VideoFileClip(\"../project_video.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"../project_video_out.mp4\">4\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">4\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.\n",
    "\n",
    "The pipeline successfully detected the lane area of the project video. During the implemetation, there are some problems that I faced in my implementation.\n",
    "- I used color transforms and gradients to create a thresholded binary image. There are difficult extract clear lines in the images with large and dark shadow or image with very bright road. I used B-channel from LAB color space to extract yellow line and L-channel to extract white line. In addition, I combined gradient w.r.t direction x, gradient magnitude and gradient direction to get more sufficient edges. \n",
    "- I manually defined fixed src and dst points for the perspective transform which based on a test image with the straight lane. Sometimes, the perspective transform did not give a good result since the car run off the center of the lane and gave different view. For further improvement, I might develop auto-detection src and dst points algorithm for the perspective transform.\n",
    "- Currently, I always did the blind search to detect lane pixels of frames. This made the lane boundary a litle bit fluctate when running on multiple frame. For further improvement, I will apply the lane pixel search based on the lane position of previous frames or using frame averaging for smooth lane detection.\n",
    "- Currently, the combined thresholded binary image still contained noisy pixels of the sides of the lane. I might apply the region of interest technique, which will process the image inside the wanted region only.\n",
    "\n",
    "###Discussion\n",
    "\n",
    "####1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
